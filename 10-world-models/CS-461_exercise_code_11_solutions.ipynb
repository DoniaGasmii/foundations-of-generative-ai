{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d46LONhmmXl"
      },
      "source": [
        "<table style=\"background-color:#FFFFFF\">   \n",
        "  <tr>     \n",
        "  <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Logo_EPFL_2019.svg\" width=\"150x\"/>\n",
        "  </td>     \n",
        "  <td>\n",
        "  <h1> <b>CS-461: Foundation Models and Generative AI</b> </h1>\n",
        "  Prof. Charlotte Bunne  \n",
        "  </td>   \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ_80cV_m3lS"
      },
      "source": [
        "# ðŸ“š  Exercise Session (Coding Part) - 11\n",
        "\n",
        "Overview of the coding part:\n",
        "\n",
        "* [**TASK 1:** Implementing Joint-Embedding Predictive Architecture (JEPA)](#task_name_1)\n",
        "    - [Subtask A: I-JEPA](#subtask_name_1_A)\n",
        "    - [Subtask B: V-JEPA](#subtask_name_1_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3La8bp6p6HM"
      },
      "source": [
        "Let's import some libraries that we will often use during the session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsNNojCMp5HE"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "from utils import * # contains helper functions for ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um6l_S3poT0F"
      },
      "source": [
        "<a name=\"task_name_1\"></a>\n",
        "## Task 1: Implementing Joint-Embedding Predictive Architecture (JEPA)\n",
        "\n",
        "In this exercise, we will look into the implementation details of JEPA. We will start from I-JEPA([Link to paper](https://arxiv.org/pdf/2301.08243)) and then move forward to V-JEPA ([Link to paper](https://arxiv.org/pdf/2404.08471)). We won't be training actual models since they typically require to be trained on ImageNet1k or large amount of video frames to get meaningful results, but the code should give you a clear, hands-on understanding of how these architectures are implemented and trained in practice.\n",
        "\n",
        "**Background**: \n",
        "- I-JEPA aims to learn **static** visual representations. It learns the representations from **images**, by predicting the latent representation of masked regions in the same image. Due to the absence of any temporal modeling, it cannot model how the world evolves, but the same underlying idea can scale from static images to videos.\n",
        "- Extending from I-JEPA, V-JEPA trains on **video** frames. The model must infer the underlying state of the world that drives video evolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwlDta7ZpL8g"
      },
      "source": [
        "<a name=\"subtask_name_1_A\"></a>\n",
        "### Task 1.A: I-JEPA\n",
        "\n",
        "<img src=\"https://scontent-zrh1-1.xx.fbcdn.net/v/t39.2365-6/353824985_215991878033819_2765220267220815437_n.png?_nc_cat=106&ccb=1-7&_nc_sid=e280be&_nc_ohc=C35AdiBHEPIQ7kNvwG9_1L1&_nc_oc=AdnV-LWXrT7MAdYsfPcR1eEmxwRoX_RlmXeTjCmUWIJ81DvjYyPbNfxeDZ3V2Mf7agKE1kK7Gmr0bgejZj5GOawF&_nc_zt=14&_nc_ht=scontent-zrh1-1.xx&_nc_gid=keNBHgcG3wwfMSPpHGMONA&oh=00_AfgBTCoNceDfXZNOf9Ijb6rQcogZkjCEpbfG_4xpJnUajg&oe=694127D1\" width=\"800\">\n",
        "\n",
        "I-JEPA can be broken into the following modules:\n",
        "1.\tContext Encoder (Neural network)\n",
        "2.\tTarget Encoder (EMA updated version of the context encoder)\n",
        "3.\tPredictor (Another neural network)\n",
        "4.\tLoss Function (Dashed lines shown in the plot. E.g. L1 or L2 distance)\n",
        "5.  Masking strategy (Produce context and target from the image)\n",
        "\n",
        "Let's look into them one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Context Encoder\n",
        "The context encoder is typically implemented as a classic vision Transformer (ViT), which we have seen previously, in exercise 7 for example. It contains:\n",
        "- **Patch Embedding**: 224Ã—224 image will be cut into 16Ã—16 patches with a 16 patch size.\n",
        "- **Position Embeddings**: Position embedding for each patch\n",
        "- **Transformer Blocks**: Self-attention + MLP layers\n",
        "- **Masking function**: Only keep the context part that will be forwarded into the context encoder. \n",
        "\n",
        "**TODO**: \n",
        "- Implement the patch embedding, which converts an image into a sequence of patch embeddings that can be processed by a Transformer.\n",
        "- Implement the masking function. Note that here the 'masks' specify patches that we want to *keep*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.patch_shape = (img_size // patch_size, img_size // patch_size)\n",
        "        \n",
        "        # [B, C, H, W] -> [B, embed_dim, H/P, W/P]\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [B, C, H, W] -> [B, embed_dim, H/P, W/P] -> [B, embed_dim, H/P * W/P] -> [B, H/P * W/P, embed_dim]\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\"\n",
        "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: *list* of tensors containing indices of patches in [N] to keep\n",
        "    \"\"\"\n",
        "    B = x.shape[0]\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(0).unsqueeze(-1).expand(B, -1, x.size(-1))\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
        "    return torch.cat(all_x, dim=0)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                    drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=[224],\n",
        "        patch_size=16,\n",
        "        in_chans=3,\n",
        "        embed_dim=768,\n",
        "        predictor_embed_dim=384,\n",
        "        depth=12,\n",
        "        predictor_depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.0,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        init_std=0.02,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size[0],\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
        "                                            int(self.patch_embed.num_patches**.5),\n",
        "                                            cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list):\n",
        "                masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None:\n",
        "            x = apply_masks(x, masks)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To initiate a model, we simply do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7pt_vsApE2Q"
      },
      "outputs": [],
      "source": [
        "def vit_tiny(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "context_encoder = vit_tiny()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Target Encoder\n",
        "The target encoder's weights are identical to the context-encoder weights at initialization, and updated via an\n",
        "exponential moving average thereafter. We have seen the same idea in many self-supervised learning approaches so far.\n",
        "\n",
        "**TODO**: Implement EMA (a classic exercise that won't hurt to be more proficient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_encoder = copy.deepcopy(context_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ema_update(context_encoder, target_encoder, beta):\n",
        "    student_model = context_encoder.eval()\n",
        "    teacher_model = target_encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
        "            teacher_param.data.mul_(other=beta).add_(other=student_param.data, alpha=1 - beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Predictor\n",
        "The predictor is another ViT that takes the context encoder output and, conditioned on positional tokens, predicts the representations of a target block at a specific location.\n",
        "\n",
        "**TODO**: Can you tell how the predictor ViT is different from the encoder ViT?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_patches,\n",
        "        embed_dim=768,\n",
        "        predictor_embed_dim=384,\n",
        "        depth=6,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.0,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        init_std=0.02,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
        "                                                requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
        "                                                        int(num_patches**.5),\n",
        "                                                        cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        trunc_normal_(self.mask_token, std=self.init_std)\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "\n",
        "        if not isinstance(masks_x, list):\n",
        "            masks_x = [masks_x]\n",
        "\n",
        "        if not isinstance(masks, list):\n",
        "            masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the context encoder, we can initiate it by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vit_predictor(num_patches=196, **kwargs):\n",
        "    model = VisionTransformerPredictor(\n",
        "        num_patches=num_patches, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "predictor = vit_predictor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Loss Function\n",
        "The loss is simply the average distance between the predicted patch-level representations (i.e., outputs from the predictor) and the target patch-level representation (i.e., outputs from the target encoder).\n",
        "\n",
        "**TODO**: In practice, JEPA uses a Smooth L1 loss instead of an L2 loss. What are the potential benefits?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(z, h):\n",
        "    loss = F.smooth_l1_loss(z, h)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Masking strategy for the context and target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having implemented the models and loss function, now we look into how an input image is processed into an input (context) and target. \n",
        "\n",
        "**TODO**: We first need a function to get a rectangle mask given a scale (0 to 1) of the image and an aspect ratio.\n",
        "\n",
        "**Hint:** Let $H$ and $W$ be `patch_h` and `patch_w`, and let $h_m$ and $w_m$ be the mask dimensions to compute.\n",
        "\n",
        "- **Area constraint:** The mask should cover a fraction $s$ of the total area: $h_m \\cdot w_m = s \\cdot H \\cdot W$\n",
        "- **Aspect ratio constraint:** The mask should have aspect ratio $r$: $\\frac{w_m}{h_m} = r$\n",
        "\n",
        "Solve these two equations for $h_m$ and $w_m$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mask(patch_h, patch_w, scale, aspect_ratio):\n",
        "    \"\"\"\n",
        "    :param patch_h: number of patches along height\n",
        "    :param patch_w: number of patches along width\n",
        "    :param scale: scale of the mask with respect to the image (0 to 1)\n",
        "    :param aspect_ratio: aspect ratio of the mask (width / height)\n",
        "    :return: height and width of the mask\n",
        "    \"\"\"\n",
        "    mask_h = int(round(math.sqrt(scale * patch_h * patch_w / aspect_ratio)))\n",
        "    mask_w = int(round(math.sqrt(scale * patch_h * patch_w * aspect_ratio)))\n",
        "    \n",
        "    # Clamp to valid dimensions\n",
        "    mask_h = min(mask_h, patch_h)\n",
        "    mask_w = min(mask_w, patch_w)\n",
        "\n",
        "    return mask_h, mask_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The targets to be predicted by the predictor can then be obtained by sampling masks with specific height and width multiple times on the image. We will record the patch indices of each target, together with the union of all targets (which will be used in getting context batches).\n",
        "\n",
        "**TODO**: Implement code to sample mask location randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_target(patch_dim, aspect_ratio, scale, M):  \n",
        "    #get the patch dimensions\n",
        "    patch_h, patch_w = patch_dim\n",
        "    block_h, block_w = get_mask(patch_h, patch_w, scale, aspect_ratio)\n",
        "    target_patches = []\n",
        "    all_patches = []\n",
        "    for z in range(M):\n",
        "        #get a random starting patch\n",
        "        start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
        "        start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
        "        start_patch = start_patch_h * patch_w + start_patch_w\n",
        "\n",
        "        patches = []\n",
        "        #get the patches in the target block\n",
        "        for i in range(block_h):\n",
        "            for j in range(block_w):\n",
        "                patches.append(start_patch + i * patch_w + j)\n",
        "                if start_patch + i * patch_w + j not in all_patches:\n",
        "                    all_patches.append(start_patch + i * patch_w + j)\n",
        "                \n",
        "        #get the target block\n",
        "        target_patches.append(torch.tensor(patches))\n",
        "    return target_patches, all_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The context used as the input of the predictor can be obtained by sampling a mask with specific height and width on the image. To avoid the overlapping between the context and target region (which will make the prediction trivial), we additionally remove the overlapping part from the context. This means the context is not necessarily rectangular. We will record the patch indices of the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_context(patch_dim, aspect_ratio, scale, target_patches):\n",
        "    patch_h, patch_w = patch_dim\n",
        "    block_h, block_w = get_mask(patch_h, patch_w, scale, aspect_ratio)\n",
        "\n",
        "    #get a random patch\n",
        "    start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
        "    start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
        "    start_patch = start_patch_h * patch_w + start_patch_w\n",
        "\n",
        "    #get the patches in the context_block\n",
        "    patches = []\n",
        "    for i in range(block_h):\n",
        "        for j in range(block_w):\n",
        "            if start_patch + i * patch_w + j not in target_patches: #remove the target patches to avoid overlapping\n",
        "                patches.append(start_patch + i * patch_w + j)\n",
        "    context_patches = [torch.tensor(patches)]\n",
        "    return context_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Finally: Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that every component is ready, we can start writing the training loop.\n",
        "\n",
        "Note that the target blocks are obtained by masking the output of the target encoder rather than the input, unlike the context blocks. This distinction is crucial to ensure target representations of a high semantic level.\n",
        "\n",
        "**TODO**: Finish the missing part of the forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward(x, context_encoder, target_encoder, predictor, target_aspect_ratio, target_scale, context_scale, context_aspect_ratio=1, M=4):\n",
        "    \n",
        "    # sample aspect ratios and scales randomly\n",
        "    target_aspect_ratio = np.random.uniform(target_aspect_ratio[0], target_aspect_ratio[1])\n",
        "    target_scale = np.random.uniform(target_scale[0], target_scale[1])\n",
        "    context_aspect_ratio = context_aspect_ratio\n",
        "    context_scale = np.random.uniform(context_scale[0], context_scale[1])\n",
        "    patch_dim = context_encoder.patch_embed.patch_shape\n",
        "    \n",
        "    #get the target block\n",
        "    target_x = target_encoder(x) # target_x: [B, N, D]\n",
        "    target_patches, all_patches = get_target(patch_dim, target_aspect_ratio, target_scale, M) # target_patches: M lists of tensors each containing L1 patch indices, all_patches: list of all target patch indices, used for detect overlapping with context block\n",
        "    target_blocks = torch.zeros((M, target_x.shape[0], len(target_patches[0]), target_x.shape[2])) # target_blocks: [M, B, L1, D]\n",
        "    for i in range(M):\n",
        "        target_blocks[i] = target_x[:, target_patches[i], :]\n",
        "\n",
        "    #get context embedding\n",
        "    context_patches = get_context(patch_dim, context_aspect_ratio, context_scale, all_patches) # context_patches: list of tensor containing L2 patch indices in context block\n",
        "    context_x = context_encoder(x, masks=context_patches) # context_x: [B, L2, D]\n",
        "\n",
        "    #get the prediction blocks, predict each target block separately\n",
        "    prediction_blocks = predictor(context_x, masks_x=context_patches, masks=target_patches) # prediction_blocks: [M*B, L1, D]\n",
        "    \n",
        "    target_blocks = target_blocks.reshape(M*target_blocks.shape[1], target_blocks.shape[2], target_blocks.shape[3]) # target_blocks: [M*B, L1, D]\n",
        "    loss = loss_fn(prediction_blocks, target_blocks)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the forward function, the training loop simply goes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ijepa(context_encoder, target_encoder, predictor, train_loader, num_epochs=10, lr=1e-4, ema_momentum=0.996, device='cuda'):\n",
        "    context_encoder.to(device)\n",
        "    target_encoder.to(device)\n",
        "    predictor.to(device)\n",
        "    \n",
        "    # only context encoder and predictor have gradients\n",
        "    target_encoder.eval()\n",
        "    for param in target_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # set up optimizer, only optimize context encoder and predictor\n",
        "    optimizer = torch.optim.AdamW(list(context_encoder.parameters()) + list(predictor.parameters()), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        context_encoder.train()\n",
        "        predictor.train()\n",
        "        for batch_idx, (images, _) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            loss = forward(images, context_encoder, target_encoder, predictor,\n",
        "                            target_aspect_ratio=(0.75, 1.5),\n",
        "                            target_scale=(0.15, 0.2),\n",
        "                            context_scale=(0.85, 1.0),\n",
        "                            context_aspect_ratio=1,\n",
        "                            M=4)\n",
        "\n",
        "            # Gradient update for the context encoder and predictor\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # EMA update for target encoder\n",
        "            ema_update(context_encoder, target_encoder, ema_momentum)\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return context_encoder, target_encoder, predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"subtask_name_1_B\"></a>\n",
        "### Task 1.B: V-JEPA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://scontent-zrh1-1.xx.fbcdn.net/v/t39.8562-6/427979095_923374739138637_7724069779118251294_n.png?_nc_cat=108&ccb=1-7&_nc_sid=f537c7&_nc_ohc=vdqIaE4r7s0Q7kNvwFZXN46&_nc_oc=AdkjBxb7JrcpOBuBTYwjozvDcxwj7Am03LkIj2znomp8wJcLp4uyrBfBWhnl5AWCIlJs6AYx97IKjvHgJQyA1tqM&_nc_zt=14&_nc_ht=scontent-zrh1-1.xx&_nc_gid=p_U2VSLILV3sF-6HdQcqdg&oh=00_Afgejhnv74y3SbrjXD-wntUvJqzYsXtR251GqtwCB2X24Q&oe=692CE30A\" width=\"800\">\n",
        "\n",
        "Extending from I-JEPA, V-JEPA trains on videos and treats videos as 3D images. The key differences are therefore on:\n",
        "- PatchEmd2D $\\rightarrow$ PatchEmd3D\n",
        "- MaskSampling2D $\\rightarrow$ MaskSampling3D\n",
        "- Other modifications to handle 3D input including 3D positional embeddings (not included)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbed3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size=16,\n",
        "        tubelet_size=2,\n",
        "        in_chans=3,\n",
        "        embed_dim=768,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.tubelet_size = tubelet_size\n",
        "\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_channels=in_chans,\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=(tubelet_size, patch_size, patch_size),\n",
        "            stride=(tubelet_size, patch_size, patch_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        B, C, T, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the masking strategy becomes 3D, the authors also make changes on how the masks are sampled and how the context and target are defined:\n",
        "- They leverage two types of masks: \n",
        "  - Short-range masks, where they take the union of 8 randomly sampled target blocks covering 15% of each frame.\n",
        "  - Long-range masks, where they take the union of 2 randomly sampled target blocks covering 70% of each frame. \n",
        "  - In both cases, the aspect ratio for all sampled blocks is randomly chosen in the range (0.75, 1.5).\n",
        "  - In both cases, the same spatial mask is applied to the full temporal dimension.\n",
        "- The sampled mask is considered as the target, and the context is directly the complement of the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_masks(patch_dim, scale, mask_type, aspect_ratio_range=(0.75, 1.5)):\n",
        "    \"\"\"\n",
        "    Generate V-JEPA target and context masks.\n",
        "    \n",
        "    Args:\n",
        "        patch_dim: Tuple of (num_frames, patch_height, patch_width)\n",
        "        scale: Fraction of each frame to mask (0.15 for short, 0.70 for long)\n",
        "        mask_type: \"short\" for short-range (8 blocks, 15% each) or \n",
        "                   \"long\" for long-range (2 blocks, 70% each)\n",
        "        aspect_ratio_range: Range for random aspect ratio sampling\n",
        "    \n",
        "    Returns:\n",
        "        target_patches: List containing tensor of masked patch indices\n",
        "        context_patches: List containing tensor of context patch indices\n",
        "    \"\"\"\n",
        "    patch_t, patch_h, patch_w = patch_dim\n",
        "    num_patches_per_frame = patch_h * patch_w\n",
        "    total_patches = patch_t * num_patches_per_frame\n",
        "    \n",
        "    # Set parameters based on mask type\n",
        "    if mask_type == \"short\":\n",
        "        num_blocks = 8\n",
        "        assert scale == 0.15, \"For 'short' mask type, scale should be 0.15\"\n",
        "    else:  # long\n",
        "        num_blocks = 2\n",
        "        assert scale == 0.70, \"For 'long' mask type, scale should be 0.70\"\n",
        "    \n",
        "    # Collect all target patches (union of blocks)\n",
        "    target_set = set()\n",
        "    \n",
        "    for _ in range(num_blocks):\n",
        "        # Sample aspect ratio randomly for each block (per paper)\n",
        "        min_ar, max_ar = aspect_ratio_range\n",
        "        aspect_ratio = min_ar + torch.rand(1).item() * (max_ar - min_ar)\n",
        "        \n",
        "        # Calculate block dimensions for a single frame\n",
        "        num_patches_block = int(num_patches_per_frame * scale)\n",
        "        block_h = int(round(math.sqrt(num_patches_block * aspect_ratio)))\n",
        "        block_w = int(round(math.sqrt(num_patches_block / aspect_ratio)))\n",
        "        \n",
        "        # Clamp to valid dimensions\n",
        "        block_h = min(block_h, patch_h)\n",
        "        block_w = min(block_w, patch_w)\n",
        "        \n",
        "        # Random starting position (same for all frames - creating a tube)\n",
        "        start_h = torch.randint(0, max(1, patch_h - block_h + 1), (1,)).item()\n",
        "        start_w = torch.randint(0, max(1, patch_w - block_w + 1), (1,)).item()\n",
        "        \n",
        "        # Add patches from all frames (tube spanning full temporal dimension)\n",
        "        for t in range(patch_t):\n",
        "            frame_offset = t * num_patches_per_frame\n",
        "            for i in range(block_h):\n",
        "                for j in range(block_w):\n",
        "                    patch_idx = frame_offset + (start_h + i) * patch_w + (start_w + j)\n",
        "                    target_set.add(patch_idx)\n",
        "    \n",
        "    # Context is the complement of target\n",
        "    all_patches = set(range(total_patches))\n",
        "    context_set = all_patches - target_set\n",
        "    \n",
        "    target_patches = [torch.tensor(sorted(target_set), dtype=torch.long)]\n",
        "    context_patches = [torch.tensor(sorted(context_set), dtype=torch.long)]\n",
        "    \n",
        "    return target_patches, context_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Final remarks \n",
        "\n",
        "That's all for this exercise! If you are interested in this topic, feel free to read V-JEPA2 ([Link to paper](https://arxiv.org/pdf/2506.09985)) and think about: what is new compared to V-JEPA ([Link to paper](https://arxiv.org/pdf/2404.08471))?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
