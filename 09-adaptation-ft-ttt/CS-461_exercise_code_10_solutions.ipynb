{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e3f095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "torch\n",
    "datasets==3.6.0\n",
    "transformers\n",
    "rouge_score\n",
    "accelerate\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3741e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\n",
      "datasets==3.6.0\n",
      "transformers\n",
      "rouge_score\n",
      "accelerate\n",
      "evaluate\n",
      "Requirement already satisfied: torch in /opt/conda/envs/poagents/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
      "Requirement already satisfied: datasets==3.6.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/poagents/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.52.4)\n",
      "Requirement already satisfied: rouge_score in /opt/conda/envs/poagents/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/poagents/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/envs/poagents/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (0.32.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from datasets==3.6.0->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 3)) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->-r requirements.txt (line 2)) (1.1.3)\n",
      "Requirement already satisfied: absl-py in /opt/conda/envs/poagents/lib/python3.12/site-packages (from rouge_score->-r requirements.txt (line 4)) (2.3.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/poagents/lib/python3.12/site-packages (from rouge_score->-r requirements.txt (line 4)) (3.9.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from rouge_score->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/poagents/lib/python3.12/site-packages (from accelerate->-r requirements.txt (line 5)) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 2)) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: click in /opt/conda/envs/poagents/lib/python3.12/site-packages (from nltk->rouge_score->-r requirements.txt (line 4)) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/poagents/lib/python3.12/site-packages (from nltk->rouge_score->-r requirements.txt (line 4)) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/poagents/lib/python3.12/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 2)) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924ef4e",
   "metadata": {},
   "source": [
    "# Part 1: Exploring summarization using in-context learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe366d5",
   "metadata": {},
   "source": [
    "In-context learning (ICL) refers to the ability of large language models to perform new tasks simply by being shown examples or instructions within the prompt, without updating their weights. Instead of traditional training, the model conditions on the pattern provided in the context window—such as a few input–output pairs, a task description, or a chain-of-thought demonstration—and dynamically adapts its behavior for the duration of the interaction. This emerges from the model’s internal representations learned during pretraining, allowing it to infer latent structure, align with new tasks, and generalize on-the-fly. In practice, ICL enables powerful “prompt-based programming,” where users can guide the model toward specific behaviors without fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3f619",
   "metadata": {},
   "source": [
    "In part 1 of this exercise, you will explore how in-context learning affects summarization performance on the XSum dataset. You will select a small subset of XSum articles and evaluate a large language model’s summaries under different prompt settings: (1) no example (zero-shot), (2) one example (one-shot), and (3) multiple examples (few-shot). For each case, you can compare the model-generated summary with the expert summary, and compare how the model’s outputs change with increasing contextual examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b3b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7801731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cad0b8cef6c45b88b6c8977cf243b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Qwen3-1.7B model\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb6c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PreTrainedTokenizerFast\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d8072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation_pipeline(prompt: str, model: PreTrainedModel, tokenizer: PreTrainedTokenizerFast,\n",
    "                             max_new_tokens: int, enable_thinking: bool, system_prompt: str | None = None, **kwargs) -> Tuple[str, str]:\n",
    "    \"\"\" \n",
    "    Text-generation pipeline for the Qwen3-model along with the tokenizer. \n",
    "    Use the Qwen3-1.7B model card to fill in this function.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for text generation.\n",
    "        model (PreTrainedModel): The pre-trained model for text generation.\n",
    "        tokenizer (PreTrainedTokenizerFast): The tokenizer associated with the model.\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing the generated content and thinking content.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt is not None:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        )\n",
    "    messages.append(\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs, \n",
    "        max_new_tokens=max_new_tokens\n",
    "    ) # type: ignore\n",
    "    output_ids = generated_ids[0][len(model_inputs[\"input_ids\"][0]):].tolist()\n",
    "    # parse thinking content\n",
    "    thinking_content = \"\"\n",
    "    index = 0\n",
    "    if enable_thinking:\n",
    "        rindex = 151668 # </think>\n",
    "        try:\n",
    "            index = len(output_ids) - output_ids[::-1].index(rindex) \n",
    "        except ValueError:\n",
    "            index =  0\n",
    "        print(index)\n",
    "        thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    return content, thinking_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc2f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "('Generated Content:\\n'\n",
      " ' In-context learning refers to the ability of large language models (LLMs) '\n",
      " 'to understand and generate responses based on the specific context provided '\n",
      " 'in the input text. Unlike traditional training methods that rely on '\n",
      " 'pre-stored knowledge, these models can process the entire conversation or '\n",
      " 'passage, allowing them to adapt their responses dynamically to the current '\n",
      " 'context. This enables seamless interaction, such as continuing a dialogue or '\n",
      " \"answering questions within a longer text, while leveraging the model's \"\n",
      " 'training on vast datasets. However, the amount of context included in a '\n",
      " 'single input is limited, which can affect the quality of the response.')\n",
      "('\\n'\n",
      " 'Thinking Content:\\n'\n",
      " ' <think>\\n'\n",
      " 'Okay, the user is asking for a short description of in-context learning for '\n",
      " 'LLMs. Let me start by recalling what I know. In-context learning refers to '\n",
      " 'the ability of large language models to understand and generate responses '\n",
      " 'based on the context provided in the input text. \\n'\n",
      " '\\n'\n",
      " \"First, I should explain that it's a key feature of modern LLMs. They don't \"\n",
      " 'just rely on pre-trained knowledge but can process the entire conversation '\n",
      " 'or passage given. This means that even if the model is trained on vast '\n",
      " 'amounts of data, it can use the specific context in the input to generate '\n",
      " 'appropriate responses.\\n'\n",
      " '\\n'\n",
      " 'I need to mention that this is different from traditional training methods '\n",
      " 'where the model is trained on a fixed dataset. In-context learning allows '\n",
      " 'the model to adapt its responses dynamically based on the input. For '\n",
      " \"example, if a user asks a question that's part of a longer conversation, the \"\n",
      " 'model can continue the dialogue seamlessly.\\n'\n",
      " '\\n'\n",
      " \"Also, it's important to note that this capability is crucial for \"\n",
      " 'applications like chatbots, customer service, and interactive systems where '\n",
      " 'the model needs to maintain context over multiple turns. However, there are '\n",
      " 'limitations, such as the amount of context that can be included in a single '\n",
      " 'input, which might affect the quality of the response.\\n'\n",
      " '\\n'\n",
      " 'I should keep the explanation concise, focusing on the main points: '\n",
      " 'context-awareness, dynamic response generation, and applications. Avoid '\n",
      " 'technical jargon but make sure the key concepts are clear. Let me check if '\n",
      " \"there's anything else the user might need. They might be looking for how it \"\n",
      " 'works or its benefits, but since the question is straightforward, a brief '\n",
      " 'definition should suffice.\\n'\n",
      " '</think>')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "example_prompt = \"Give me a short description of in-context learning for LLMs.\"\n",
    "generated_content, thinking_content = text_generation_pipeline(\n",
    "    prompt=example_prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=5000,\n",
    "    enable_thinking=True,\n",
    "    system_prompt=\"You are a helpful assistant that helps people find information.\"\n",
    ")\n",
    "\n",
    "pprint(f\"Generated Content:\\n {generated_content}\")\n",
    "pprint(f\"\\nThinking Content:\\n {thinking_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e23dd",
   "metadata": {},
   "source": [
    "### Exploring the Xsum dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ee333",
   "metadata": {},
   "source": [
    "The xsum dataset (extreme summarization), introduced in [1], aims to create a short, one-sentence news summary answering the question “What is the article about?”.\n",
    "\n",
    "\n",
    "> [1] Narayan, S., Cohen, S. B., & Lapata, M. (2018). Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "febff767",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_dataset = load_dataset(\"EdinburghNLP/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b869310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'The ex-Reading defender denied fraudulent trading charges '\n",
      "             'relating to the Sodje Sports Foundation - a charity to raise '\n",
      "             'money for Nigerian sport.\\n'\n",
      "             'Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, '\n",
      "             'Bright, 50 and Stephen, 42.\\n'\n",
      "             'Appearing at the Old Bailey earlier, all four denied the '\n",
      "             'offence.\\n'\n",
      "             'The charge relates to offences which allegedly took place '\n",
      "             'between 2008 and 2014.\\n'\n",
      "             'Sam, from Kent, Efe and Bright, of Greater Manchester, and '\n",
      "             'Stephen, from Bexley, are due to stand trial in July.\\n'\n",
      "             'They were all released on bail.',\n",
      " 'id': '38295789',\n",
      " 'summary': 'Former Premier League footballer Sam Sodje has appeared in court '\n",
      "            'alongside three brothers accused of charity fraud.'}\n"
     ]
    }
   ],
   "source": [
    "# explore an example \n",
    "from pprint import pprint \n",
    "split = \"validation\"\n",
    "pprint(xsum_dataset[split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb009e",
   "metadata": {},
   "source": [
    "We see that the summary is concise, makes sense with the article. But how does one evaluate such summaries?\n",
    "\n",
    "For this, we work as follows: <br>\n",
    "A sequence $Z = [z_1, \\dots, z_n]$ is a subsequence of $X = [x_1, \\dots, x_m]$ if $\\exists$ a strictly increasing sequence $[i_1, \\dots, i_k]$ of indices such that $\\forall j = 1, \\dots, k$, we have $x_{i_j} = z_j$. Given two sequences $X$ and $Y$, the longest common subsequence (LCS) of $X$ and $Y$ is a common subsequence with maximum length (i.e. $\\text{LCS}(X,Y) = \\max\\{k: Z\\text{ is a common subsequence of }X\\text{ and }Y\\}$). Consider the summary sentence of the article as a sequence of words. We can hypothesize that the longer the LCS of two summary sentences, the more similar the two summaries are.\n",
    "\n",
    " We will design a metric around this as follows: Let $X$ be a $m$-length summary generated by an expert, and $Y$ be a $n$-length candidate summary sentence generated by a neural network. Let $$R_{lcs} = \\frac{|\\text{LCS}(X,Y)|}{m}, \\quad P_{lcs} = \\frac{|\\text{LCS}(X,Y)|}{n}, \\quad F_{lcs} = \\frac{(1+\\beta^2) R_{lcs} P_{lcs}}{R_{lcs} + \\beta^2 P_{lcs}}$$ \n",
    "\n",
    " be the F-measure. \n",
    "> Think: <br>(a) This would remind you of the $F_\\beta$ score. How are the recall, precision in $F_\\beta$ related to the LCS here?<br>(b) What is the maximum and minimum values $F_{lcs}$ can take and when? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0f9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_LCS(expert_summary: str, candidate_summary: str, beta: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Compute the F-measure based on the Longest Common Subsequence (LCS) between\n",
    "    an expert summary and a candidate summary. First, we tokenize both summaries\n",
    "    into words (removing punctuation and converting to lowercase). Then, we compute\n",
    "    the LCS length and use it to calculate recall, precision, and the F-measure.\n",
    "\n",
    "    Args:\n",
    "        expert_summary (str): The expert-generated summary.\n",
    "        candidate_summary (str): The candidate summary generated by a model.\n",
    "        beta (float): The weight of recall in the F-measure calculation.\n",
    "    Returns:\n",
    "        float: The F-measure based on LCS.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    def toks(s):\n",
    "        return re.findall(r\"\\w+\", s.lower())\n",
    "\n",
    "    expert_toks   = toks(expert_summary)\n",
    "    candidate_toks = toks(candidate_summary)\n",
    "\n",
    "    def LCS(X: list, Y: list) -> int:\n",
    "        m = len(X)\n",
    "        n = len(Y)\n",
    "        L = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(m + 1):\n",
    "            for j in range(n + 1):\n",
    "                if i == 0 or j == 0:\n",
    "                    L[i][j] = 0\n",
    "                elif X[i - 1] == Y[j - 1]:\n",
    "                    L[i][j] = L[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "        return L[m][n]\n",
    "    lcs_length = LCS(expert_toks, candidate_toks)\n",
    "    m = len(expert_toks)\n",
    "    n = len(candidate_toks)\n",
    "    R_lcs = lcs_length / m if m > 0 else 0.0\n",
    "    P_lcs = lcs_length / n if n > 0 else 0.0\n",
    "    if R_lcs + beta**2 * P_lcs == 0:\n",
    "        return 0.0\n",
    "    F_lcs = (1 + beta**2) * R_lcs * P_lcs / (R_lcs + beta**2 * P_lcs)\n",
    "    return F_lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fceeecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_expert_sentence = \"Artificial intelligence is transforming healthcare by enabling earlier and more accurate diagnoses.\"\n",
    "random_candidate_sentence = \"AI is revolutionizing medicine by making diagnostic processes faster and more precise.\"\n",
    "\n",
    "f_lcs_score = F_LCS(random_expert_sentence, random_candidate_sentence, beta=1.0)\n",
    "assert np.isclose(f_lcs_score, 1/3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abae06e",
   "metadata": {},
   "source": [
    "> The above $F$-measure is defined for single sentences. Think of how one can extend it to multiple sentences? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed1abd",
   "metadata": {},
   "source": [
    "Now you write a function, which takes in a document to summarize, along with a parameter `n_ICL` which controls the number of examples to put in-context. You should take these examples from the training subset of `xsum_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ae5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(document: str, n_ICL: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary for the given document using in-context learning with n_ICL examples.\n",
    "\n",
    "    Args:\n",
    "        document (str): The document to summarize.\n",
    "        n_ICL (int): The number of in-context learning examples to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary.\n",
    "    \"\"\"\n",
    "    # Select n_ICL examples from the training subset of xsum_dataset\n",
    "    train_data = xsum_dataset[\"train\"]\n",
    "    examples = train_data.select(range(n_ICL))\n",
    "\n",
    "    # Construct the prompt with in-context examples\n",
    "    prompt = \"\"\n",
    "    for example in examples:\n",
    "        prompt += f\"Document: {example['document']}\\nSummary: {example['summary']}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Document: {document}\\nSummary:\"\n",
    "\n",
    "    # Generate the summary using the text generation pipeline\n",
    "    generated_summary, _ = text_generation_pipeline(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=128,\n",
    "        enable_thinking=False,\n",
    "        system_prompt=\"You are a helpful assistant that summarizes documents. You might be given some examples to help you.\"\\\n",
    "        \" You should generate a concise and relevant summary for the provided document. Just provide the summary for the document without any additional text.\"\n",
    "    )\n",
    "\n",
    "    return generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431e8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to evaluate ICL\n",
    "from tqdm import tqdm\n",
    "def evaluate_ICL(validation_data, n_ICL: int) -> Tuple[float, list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Evaluate the in-context learning performance on the validation dataset using F-LCS metric.\n",
    "\n",
    "    Args:\n",
    "        validation_data: Subset of the validation dataset containing documents and expert summaries.\n",
    "        n_ICL (int): The number of in-context learning examples to use.\n",
    "\n",
    "    Returns:\n",
    "        float: The average F-LCS score over the validation dataset.\n",
    "    \"\"\"\n",
    "    f_lcs_scores = []\n",
    "    generated_summaries = []\n",
    "    expert_summaries = []\n",
    "    for example in tqdm(validation_data, leave=False):\n",
    "        document, summary = example['document'], example['summary']\n",
    "        generated_summary = generate_summary(document, n_ICL)\n",
    "        generated_summaries.append(generated_summary)\n",
    "        expert_summaries.append(summary)\n",
    "        f_lcs_score = F_LCS(summary, generated_summary, beta=1.0)\n",
    "        f_lcs_scores.append(f_lcs_score)\n",
    "    # auto_rouge_scores = rouge.compute(predictions=generated_summaries, references=expert_summaries, rouge_types=[\"rougeL\"])[\"rougeL\"]\n",
    "    # return auto_rouge_scores, generated_summaries, expert_summaries\n",
    "    return np.mean(f_lcs_scores), generated_summaries, expert_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25572498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-LCS Score with 0 ICL examples: 0.1064\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-LCS Score with 1 ICL examples: 0.1448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-LCS Score with 2 ICL examples: 0.1559\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-LCS Score with 3 ICL examples: 0.1625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-LCS Score with 4 ICL examples: 0.2128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validation_data = xsum_dataset[\"validation\"].select(range(6))\n",
    "\n",
    "for n_ICL in range(5):\n",
    "    f_lcs_score, _, _ = evaluate_ICL(validation_data, n_ICL)\n",
    "    print(f'F-LCS Score with {n_ICL} ICL examples: {f_lcs_score:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44882de1",
   "metadata": {},
   "source": [
    "# Part 2: Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f85ecc",
   "metadata": {},
   "source": [
    "Neural networks are usually trained and stored in float32 (FP32). Quantization means representing numbers with lower precision, like int8 (8-bit integers). \n",
    "\n",
    "This is done to:\n",
    "- Lower memory footprint\n",
    "- Decrease inference costs (less bandwidth, higher throughput)\n",
    "- Allow deployment on constrained hardware (mobile, edge-devices etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a557f6",
   "metadata": {},
   "source": [
    "In this part, we will take a look at uniform affine INT8 quantization. However, after attempting this, you are encouraged to deep dive further into understanding recent quantization schemes (GGUF etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d3f96",
   "metadata": {},
   "source": [
    "Consider a real tensor $\\mathbf{x} \\in \\mathbb{R}^{\\bullet}$ which we want to map to a 8-bit integer tensor $q$. We can do so as: $$q = \\text{clamp}\\left(\\text{round}\\left(\\frac{x}{s}\\right) + z, q_{\\min}, q_{\\max}\\right)$$ where $s$ is a scale parameter, $z$ is the zero-point integer, and $q_{\\min}$, $q_{\\max}$ are the range limits. \n",
    "\n",
    "> What are the range limits for signed INT8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_min = -128\n",
    "q_max = 127"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45881878",
   "metadata": {},
   "source": [
    "> How would we get back the de-quantized variable? Fill: $$\\hat{x} = s(q-z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6d389",
   "metadata": {},
   "source": [
    "In this exercise, we will set the zero-point $z=0$, implementing symmetric quantization. \n",
    "> What do you think is the effect of zero-point? Can $z$ be non-zero? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c0a37",
   "metadata": {},
   "source": [
    "Depending on the accuracy / latency trade-off you are targeting, we can have different granularity of quantization parameters:\n",
    "- *Per-tensor quantization*: You will have one pair of $(s,z)$ per tensor\n",
    "- *Per-channel quantization*: You will store a pair of $(s,z)$ per element along one of the dimensions of the tensor (for eg., a tensor with shape $[B,C,H,W]$ can have $C$ pairs of $(s,z)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f3597",
   "metadata": {},
   "source": [
    "In this exercise, we work with Post-training quantization (PTQ) where we take a trained model, keep weights fixed, and:\n",
    "- Compute scale / zero-point from real weights (and optionally activations).\n",
    "- Replace FP weights with INT8 + scale/zero-point.\n",
    "- Run inference with the quantized model.\n",
    "\n",
    "Benefits: simple, no retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b7ec4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa5dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class QuantizationParams:\n",
    "    scale: torch.Tensor       # shape: [] or [C] for per-channel\n",
    "    zero_point: torch.Tensor  # same shape as scale\n",
    "    qmin: int\n",
    "    qmax: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "636e46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_symmetric_params(\n",
    "        x: torch.Tensor,\n",
    "        num_bits: int,\n",
    "        per_channel: bool = False,\n",
    "        ch_axis: Optional[int] = None\n",
    ") -> QuantizationParams:\n",
    "    \"\"\"\n",
    "    Compute symmetric quantization parameters for x.\n",
    "\n",
    "    Symmetric means:\n",
    "        - Representable range is approximately [-x_max, x_max]\n",
    "        - zero_point = 0\n",
    "\n",
    "    If per_channel is True, compute separate scales per channel along ch_axis.\n",
    "\n",
    "    TODO:\n",
    "        1. Compute max absolute value (per tensor or per channel).\n",
    "        2. Avoid division-by-zero when tensor is all zeros (e.g. set scale=1.0).\n",
    "        3. Compute scale = max_val / qmax,\n",
    "        4. Set zero_point = 0.\n",
    "    \"\"\"\n",
    "    # calculate qmin and qmax based on num_bits\n",
    "    q_min = - (2 ** (num_bits - 1)) # ... \n",
    "    q_max = (2 ** (num_bits - 1)) - 1 # ...\n",
    "\n",
    "    if per_channel:\n",
    "        # reduce over all dims except ch_axis\n",
    "        dims = [d for d in range(x.dim()) if d != ch_axis]\n",
    "        max_val = x.abs().amax(dim=dims)\n",
    "    else:\n",
    "        max_val = x.abs().amax()\n",
    "\n",
    "    \n",
    "    # TODO: handle the all-zero case to avoid scale = 0\n",
    "    # Hint: use torch.where or a small epsilon.\n",
    "\n",
    "    # ====== YOUR CODE HERE ======\n",
    "    # compute max_val, scale, zero_point\n",
    "    eps = 1e-8\n",
    "    max_val = torch.clamp(max_val, min=eps)\n",
    "    scale = max_val / q_max\n",
    "    zero_point = torch.zeros_like(scale)\n",
    "\n",
    "    # ====== END YOUR CODE =======\n",
    "    return QuantizationParams(\n",
    "        scale=scale,\n",
    "        zero_point=zero_point,\n",
    "        qmin=q_min,\n",
    "        qmax=q_max\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3784a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(\n",
    "    x: torch.Tensor,\n",
    "    params: QuantizationParams,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Quantize the input tensor x using the provided quantization parameters.\n",
    "    Formula:\n",
    "        q = clamp(round(x / scale) + zero_point, qmin, qmax)\n",
    "\n",
    "    Broadcasting rules:\n",
    "        - If scale/zero_point are per-channel, make sure they broadcast\n",
    "          correctly along the tensor dimensions (e.g. [C, 1] for [C, D] weights).\n",
    "\n",
    "    TODO:\n",
    "        1. Reshape scale and zero_point for broadcasting if needed.\n",
    "        2. Apply the affine transform + rounding + clamping.\n",
    "        3. Cast to torch.int8.\n",
    "    \"\"\" \n",
    "    scale = params.scale \n",
    "    zp = params.zero_point\n",
    "\n",
    "    # ====== YOUR CODE HERE ======\n",
    "    # calculate the quantized tensor q\n",
    "    while scale.dim() < x.dim():\n",
    "        scale = scale.view(*scale.shape, *([1] * (x.dim() - scale.dim())))\n",
    "        zp = zp.view(*zp.shape, *([1] * (x.dim() - zp.dim())))\n",
    "\n",
    "    q = torch.round(x / scale) + zp\n",
    "    q = torch.clamp(q, params.qmin, params.qmax)\n",
    "    q = q.to(torch.int8)\n",
    "    \n",
    "\n",
    "    # ====== END YOUR CODE ======= \n",
    "    return q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08a4d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize_tensor(\n",
    "        q: torch.Tensor,\n",
    "        params: QuantizationParams,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dequantize int8 tensor back to float using the formula you derived above.\n",
    "\n",
    "    TODO:\n",
    "        1. Handle broadcasting same as in quantize_tensor.\n",
    "        2. Return float32 tensor.\n",
    "    \"\"\"\n",
    "    scale = params.scale \n",
    "    zp = params.zero_point\n",
    "\n",
    "    # ====== YOUR CODE HERE ======\n",
    "    # calculate the dequantized tensor x_hat\n",
    "    while scale.dim() < q.dim():\n",
    "        scale = scale.view(*scale.shape, *([1] * (q.dim() - scale.dim())))\n",
    "        zp = zp.view(*zp.shape, *([1] * (q.dim() - zp.dim())))\n",
    "\n",
    "    x_hat = scale * (q.to(torch.float32) - zp)\n",
    "\n",
    "\n",
    "    # ====== END YOUR CODE =======\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1d9a3",
   "metadata": {},
   "source": [
    "Now we are ready to write the class for a INT8-quantized linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35d5fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight-only INT8 quantized Linear layer.\n",
    "\n",
    "    - Stores weights as int8 + (scale, zero_point).\n",
    "    - Bias stays in float. \n",
    "    - On forward, dequantizes weights to float and uses a regular matmul.\n",
    "\n",
    "    This is *not* optimized for speed, but shows how weight-only PTQ works.\n",
    "\n",
    "    Usage:\n",
    "        qlinear = QuantLinear.from_fp_module(linear, weight_bits=8, per_channel=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        weight_bits: int = 8,\n",
    "        per_channel: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_bits = weight_bits\n",
    "        self.per_channel = per_channel\n",
    "\n",
    "        # These will be filled by from_fp_module\n",
    "        self.register_buffer(\"weight_int8\", torch.empty(out_features, in_features, dtype=torch.int8))\n",
    "        self.register_buffer(\"scale\", torch.ones(out_features))\n",
    "        self.register_buffer(\"zero_point\", torch.zeros(out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    @classmethod\n",
    "    def from_fp_module(\n",
    "        cls,\n",
    "        linear: nn.Linear,\n",
    "        weight_bits: int = 8,\n",
    "        per_channel: bool = True,\n",
    "    ) -> \"QuantLinear\":\n",
    "        \"\"\"\n",
    "        Create a QuantLinear from a pretrained nn.Linear by\n",
    "        quantizing its weights.\n",
    "\n",
    "        TODO:\n",
    "            1. Instantiate QuantLinear with appropriate sizes.\n",
    "            2. Compute per-channel quantization parameters for weight:\n",
    "                   shape [out_features, in_features]\n",
    "               with channel axis = 0 (each output neuron has its own scale).\n",
    "            3. Quantize weights and store in weight_int8, scale, zero_point.\n",
    "            4. Copy bias if it exists.\n",
    "        \"\"\"\n",
    "        qlin = cls(\n",
    "            in_features=linear.in_features,\n",
    "            out_features=linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            weight_bits=weight_bits,\n",
    "            per_channel=per_channel,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w = linear.weight.data\n",
    "\n",
    "            # ====== YOUR CODE HERE ======\n",
    "            params = calc_symmetric_params(\n",
    "                w,\n",
    "                num_bits=weight_bits,\n",
    "                per_channel=per_channel,\n",
    "                ch_axis=0,\n",
    "            )\n",
    "            q_w = quantize_tensor(w, params)\n",
    "\n",
    "            qlin.weight_int8.copy_(q_w)\n",
    "            qlin.scale.copy_(params.scale)\n",
    "            qlin.zero_point.copy_(params.zero_point)\n",
    "\n",
    "            if linear.bias is not None:\n",
    "                qlin.bias.copy_(linear.bias.data)\n",
    "            # ====== END YOUR CODE =======\n",
    "\n",
    "        return qlin\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "\n",
    "        TODO:\n",
    "            1. Dequantize weight_int8 using self.scale, self.zero_point.\n",
    "            2. Compute x @ W^T + bias.\n",
    "        \"\"\"\n",
    "        # ====== YOUR CODE HERE ======\n",
    "        params = QuantizationParams(\n",
    "            scale=self.scale,\n",
    "            zero_point=self.zero_point,\n",
    "            qmin=-128,\n",
    "            qmax=127,\n",
    "        )\n",
    "        w = dequantize_tensor(self.weight_int8, params).to(x.dtype)\n",
    "        out = x.matmul(w.t())\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        # ====== END YOUR CODE =======\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8876b2",
   "metadata": {},
   "source": [
    "# Example: Training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06e903ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, quantized: bool = False):\n",
    "        super().__init__()\n",
    "        self.quantized = quantized\n",
    "\n",
    "        # Define FP32 linear layers first\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "        if self.quantized:\n",
    "            # Replace with quantized versions\n",
    "            self.fc1 = QuantLinear.from_fp_module(self.fc1, weight_bits=8, per_channel=True)\n",
    "            self.fc2 = QuantLinear.from_fp_module(self.fc2, weight_bits=8, per_channel=True)\n",
    "            self.fc3 = QuantLinear.from_fp_module(self.fc3, weight_bits=8, per_channel=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3216e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataloaders(batch_size: int = 128) -> Tuple[DataLoader, DataLoader]:\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    train_len = int(0.8 * len(dataset))\n",
    "    val_len = len(dataset) - train_len\n",
    "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a706d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb3a226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a3c5a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FP32 model...\n",
      "[FP32] Epoch 1: loss=0.3777, val_acc=0.9392\n",
      "[FP32] Epoch 2: loss=0.1504, val_acc=0.9583\n",
      "[FP32] Epoch 3: loss=0.1004, val_acc=0.9684\n",
      "[INT8] Validation accuracy after PTQ: 0.9682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader, val_loader = get_mnist_dataloaders()\n",
    "\n",
    "# ====== FP32 baseline ======\n",
    "fp32_model = MLP(quantized=False).to(device)\n",
    "optimizer = torch.optim.Adam(fp32_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training FP32 model...\")\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(fp32_model, train_loader, optimizer, device)\n",
    "    acc = evaluate(fp32_model, val_loader, device)\n",
    "    print(f\"[FP32] Epoch {epoch+1}: loss={loss:.4f}, val_acc={acc:.4f}\")\n",
    "\n",
    "# Freeze weights\n",
    "fp32_model.eval()\n",
    "\n",
    "# ====== Part INT8 model (weight-only PTQ) ======\n",
    "# In a real PTQ pipeline, you'd typically train FP32, then quantize.\n",
    "# Here we re-instantiate from trained FP32 weights.\n",
    "int8_model = MLP(quantized=False)  # start as FP32\n",
    "int8_model.load_state_dict(fp32_model.state_dict())\n",
    "\n",
    "# Replace with quantized layers\n",
    "int8_model.fc1 = QuantLinear.from_fp_module(int8_model.fc1, weight_bits=8, per_channel=True)\n",
    "int8_model.fc2 = QuantLinear.from_fp_module(int8_model.fc2, weight_bits=8, per_channel=True)\n",
    "int8_model.fc3 = QuantLinear.from_fp_module(int8_model.fc3, weight_bits=8, per_channel=True)\n",
    "\n",
    "int8_model.to(device)\n",
    "\n",
    "int8_acc = evaluate(int8_model, val_loader, device)\n",
    "print(f\"[INT8] Validation accuracy after PTQ: {int8_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea2a59",
   "metadata": {},
   "source": [
    "# Now let's quantize a small FP16 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8ad787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters_in_bits(model: nn.Module, bits_per_param: int = 32) -> int:\n",
    "    \"\"\"\n",
    "    Approximate model size in *bits* assuming all parameters\n",
    "    are stored with bits_per_param bits.\n",
    "\n",
    "    This is a simplified view (ignores buffers like running stats).\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params * bits_per_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a34def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size_estimates(model: nn.Module, name: str):\n",
    "    bits_fp32 = count_parameters_in_bits(model, bits_per_param=32)\n",
    "    bits_fp16 = count_parameters_in_bits(model, bits_per_param=16)\n",
    "    bits_int8 = count_parameters_in_bits(model, bits_per_param=8)\n",
    "\n",
    "    def to_mb(bits):\n",
    "        return bits / 8 / (1024 ** 2)\n",
    "\n",
    "    print(f\"=== {name} size estimates ===\")\n",
    "    print(f\"FP32: ~{to_mb(bits_fp32):.2f} MB\")\n",
    "    print(f\"FP16: ~{to_mb(bits_fp16):.2f} MB\")\n",
    "    print(f\"INT8: ~{to_mb(bits_int8):.2f} MB\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef84257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursively_quantize_linear_modules(module: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Recursively traverse the module and replace nn.Linear with QuantLinear.\n",
    "\n",
    "    TODO:\n",
    "        1. For each child that is an nn.Linear, replace it in-place\n",
    "           with QuantLinear.from_fp_module(child).\n",
    "        2. Recurse into submodules.\n",
    "\n",
    "    Hint:\n",
    "        - Use module.named_children() and setattr(module, name, new_child)\n",
    "    \"\"\"\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # ====== YOUR CODE HERE ======\n",
    "            qchild = QuantLinear.from_fp_module(child, weight_bits=8, per_channel=True).to(child.weight.device)\n",
    "            setattr(module, name, qchild)\n",
    "            # ====== END YOUR CODE =======\n",
    "        else:\n",
    "            recursively_quantize_linear_modules(child)\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb2f5449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FP16 model Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90deb715b5164411b96c20e880ab0c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original model (FP16) size estimates ===\n",
      "FP32: ~6563.47 MB\n",
      "FP16: ~3281.74 MB\n",
      "INT8: ~1640.87 MB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1720574976"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"  # larger LLM\n",
    "print(f\"Loading FP16 model {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "print_model_size_estimates(model_fp16, name=\"Original model (FP16)\")\n",
    "\n",
    "sum(p.numel() for p in model_fp16.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1afdac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FP16 sample ===\n",
      "Once upon a time in a world of quantized models,  \n",
      "There lived a being named Alex,  \n",
      "A curious mind, with a passion for code,  \n",
      "And a deep love for the art of machine learning.\n",
      "\n",
      "Alex was no ordinary programmer—  \n",
      "They were a pioneer in the realm of quantized models,  \n",
      "A bridge between the abstract and the real,  \n",
      "Where precision and simplicity met.\n",
      "\n",
      "In a world where models were vast and complex,  \n",
      "Alex sought to make them light and true,  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple sanity check generation\n",
    "prompt = \"Once upon a time in a world of quantized models,\"\n",
    "print(\"=== FP16 sample ===\")\n",
    "print(text_generation_pipeline(prompt, model_fp16, tokenizer, max_new_tokens=100, enable_thinking=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9c05967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720574976"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model_fp16.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d140c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing nn.Linear layers to INT8...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====== Part 3: Quantize the LLM weights to INT8 (weight-only) ======\n",
    "print(\"Quantizing nn.Linear layers to INT8...\")\n",
    "model_int8 = recursively_quantize_linear_modules(model_fp16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a9d39f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311288832"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model_int8.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0384126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INT8 sample ===\n",
      "Once upon a time in a world of quantized models, the landscape of artificial intelligence was reshaped by a revolutionary idea: **quantization**. Instead of working with continuous data, AI systems began to operate with **discrete, quantized representations**—a form of digital compression that reduced computational complexity and memory usage.\n",
      "\n",
      "In this world, neural networks were no longer built on floating-point precision but on **integer arithmetic**, where weights and activations were stored as integers. This shift allowed for **f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== INT8 sample ===\")\n",
    "print(text_generation_pipeline(prompt, model_int8, tokenizer, max_new_tokens=100, enable_thinking=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cda09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c0fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf867522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e4954c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
